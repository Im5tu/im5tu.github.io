<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>devops on CodeWithStu's Blog</title><link>https://im5tu.io/tags/devops/</link><description>Recent content in devops on CodeWithStu's Blog</description><generator>Hugo -- gohugo.io</generator><language>en-gb</language><lastBuildDate>Mon, 01 May 2023 01:00:00 +0000</lastBuildDate><atom:link href="https://im5tu.io/tags/devops/index.xml" rel="self" type="application/rss+xml"/><item><title>Observed No. 12 - Removing Pre-Commit Dependencies With Docker</title><link>https://im5tu.io/article/2023/05/observed-no.-12-removing-pre-commit-dependencies-with-docker/</link><pubDate>Mon, 01 May 2023 01:00:00 +0000</pubDate><guid>https://im5tu.io/article/2023/05/observed-no.-12-removing-pre-commit-dependencies-with-docker/</guid><description>&lt;p>Welcome to the 12th edition of Observed! The newsletter delivers a tip you can implement across many categories like AWS, Terraform and General DevOps practices in your infrastructure. This week&amp;rsquo;s edition looks how we can use Precommit with existing docker images.&lt;/p>
&lt;h2 id="what-is-pre-commit">What is Pre-Commit?&lt;/h2>
&lt;p>Pre-commit is a tool that helps developers to ensure that the code they commit is consistent with the project&amp;rsquo;s guidelines and standards. This framework allows developers to define a set of hooks or scripts to run before a commit is made to a Git repository. These hooks can perform various checks and tests, such as code formatting, syntax checking, linting, and security scanning.&lt;/p>
&lt;p>Pre-commit provides a convenient way to automate these checks and ensure that code is consistently formatted and meets the project&amp;rsquo;s quality standards. It can be configured to run automatically on every commit or manually by running a command in the terminal. Pre-commit is written in Python and is available as an open-source tool that can be used with any programming language. There are many community-built hooks for languages such as Terraform &amp;amp; .NET.&lt;/p>
&lt;h2 id="why-should-we-use-pre-commit">Why should we use Pre-Commit?&lt;/h2>
&lt;p>If you aren&amp;rsquo;t already using Pre-Commit, there are several reasons why you should consider using Pre-commit in your projects:&lt;/p>
&lt;ol>
&lt;li>Consistency: Pre-commit helps ensure that code is consistently formatted and adheres to the project&amp;rsquo;s guidelines and standards. This can make it easier for developers to read and understand code and reduce errors and bugs.&lt;/li>
&lt;li>Efficiency: Pre-commit allows developers to automate checks and tests that would otherwise need to be done manually. This can save time and reduce the risk of human error.&lt;/li>
&lt;li>Security: Before code is committed to the repository, Pre-Commit can be configured to run security checks, such as scanning for vulnerabilities or checking for sensitive data. This can help reduce the risk of security breaches.&lt;/li>
&lt;/ol>
&lt;h2 id="why-are-dependencies-a-problem-for-pre-commit">Why Are Dependencies A Problem For Pre-Commit?&lt;/h2>
&lt;p>Dependencies can be problematic for Pre-Commit because they can lead to compatibility issues or version conflicts. Pre-commit hooks are executed in a separate environment from the main project, and this environment may have different dependencies or versions of dependencies installed.&lt;/p>
&lt;p>If a hook relies on a specific version of a package or library that is not installed on the machine, it may fail to execute. Similarly, if multiple hooks require different versions of the same package, conflicts may prevent one or more hooks from running correctly.&lt;/p>
&lt;p>To avoid these issues, we can, where available, use the docker functionality of pre-commit to isolate the dependencies of our Pre-Commit hooks from the machine running the checks, resulting in a more stable and consistent output.&lt;/p>
&lt;h2 id="how-to-use-docker-images-with-pre-commit">How To Use Docker Images With Pre-Commit&lt;/h2>
&lt;p>Pre-commit hooks can be run via Docker to ensure they are executed consistently across different environments. This can be especially useful for developers who need the necessary tools or dependencies installed on their local machines.&lt;/p>
&lt;p>To use Docker with pre-commit, you&amp;rsquo;ll need to specify a Docker image for each hook in your &lt;code>.pre-commit-config.yaml&lt;/code> file. Here’s an example from &lt;a href="https://terraform-docs.io/how-to/pre-commit-hooks/">Terraform Docs&lt;/a>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">repos&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">repo&lt;/span>: &lt;span style="color:#ae81ff">local&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">hooks&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">id&lt;/span>: &lt;span style="color:#ae81ff">terraform-docs&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">terraform-docs&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">language&lt;/span>: &lt;span style="color:#ae81ff">docker_image&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">entry&lt;/span>: &lt;span style="color:#ae81ff">quay.io/terraform-docs/terraform-docs:latest &lt;/span> &lt;span style="color:#75715e"># or, change latest to pin to a specific version&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">args&lt;/span>: [&lt;span style="color:#e6db74">&amp;#34;ARGS&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;TO PASS&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;INCLUDING PATH&amp;#34;&lt;/span>] &lt;span style="color:#75715e"># e.g. [&amp;#34;--output-file&amp;#34;, &amp;#34;README.md&amp;#34;, &amp;#34;./mymodule/path&amp;#34;]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ae81ff">pass_filenames:false&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note that building a Docker image from the repo can be slow, so it is recommended to download the pre-built image instead, as shown in the example. As these are docker references, we can change the tag we wish Pre-Commit to pull, enabling easy versioning of dependencies.&lt;/p>
&lt;p>In addition to specifying the Docker image, you can pass arguments to the pre-commit hook using the args key. This can be useful for customizing the behaviour of the hook, such as specifying the output file or passing additional command-line arguments.&lt;/p>
&lt;p>To build custom docker images for Pre-Commit, check &lt;a href="https://pre-commit.com/index.html#docker">this section of the Pre-Commit documentation&lt;/a>. As mentioned above, it’s advised that you pre-build your docker images for your team&amp;rsquo;s performance.&lt;/p>
&lt;p>&lt;strong>📣 Get the Weekly Newsletter Straight to Your Inbox! 📣&lt;/strong>&lt;/p>
&lt;p>Don&amp;rsquo;t miss out on the latest updates! Subscribe to the &lt;a href="https://news.codewithstu.tv">Observed! Newsletter&lt;/a> now and stay up-to-date with the latest tips and tricks across AWS, Devops and Architecture. &lt;a href="https://news.codewithstu.tv">Click here&lt;/a> to subscribe and start receiving your weekly dose of tech news!&lt;/p></description></item><item><title>Observed No. 11 - Saving Costs on AWS</title><link>https://im5tu.io/article/2023/04/observed-no.-11-saving-costs-on-aws/</link><pubDate>Mon, 24 Apr 2023 01:00:00 +0000</pubDate><guid>https://im5tu.io/article/2023/04/observed-no.-11-saving-costs-on-aws/</guid><description>&lt;p>Welcome to the 11th edition of Observed! The newsletter delivers a tip you can implement across many categories like AWS, Terraform and General DevOps practices in your infrastructure. This week&amp;rsquo;s edition looks at AWS Cost Savings.&lt;/p>
&lt;p>Every company seems to be cutting costs in one way or another. Let’s look at different ways you can visualize and reduce costs.&lt;/p>
&lt;h2 id="aws-cost-explorer">AWS Cost Explorer&lt;/h2>
&lt;p>AWS Cost Explorer should be your first stop when analyzing costs. AWS Cost Explorer is a cost management tool that helps AWS users visualize, understand, and manage their AWS costs and usage. With Cost Explorer, users can analyze their AWS spending patterns and identify optimisation areas, helping reduce costs and increase efficiency.&lt;/p>
&lt;p>Tip: Inside cost explorer, you can group by usage type, often showing you the cause of hidden costs.&lt;/p>
&lt;h2 id="aws-budgets">AWS Budgets&lt;/h2>
&lt;p>Closely related to Cost Explorer is Budgets. AWS Budgets is a cost management tool that helps users set custom cost and usage budgets for their AWS resources, services, and accounts. With AWS Budgets, users can monitor their AWS spending and receive alerts when their usage or costs exceed the defined thresholds, helping to avoid unexpected expenses and optimize costs. This is vital for knowing that there is a problem ahead of time.&lt;/p>
&lt;h2 id="utilizing-saving-plans--reserved-instances">Utilizing Saving Plans &amp;amp; Reserved Instances&lt;/h2>
&lt;p>We can utilize Saving Plans and Reserved Instances to save costs on their AWS usage. Both options offer significant discounts compared to on-demand pricing but work slightly differently:&lt;/p>
&lt;p>Saving Plans offer flexible pricing for AWS compute usage compared to on-demand pricing. Users can commit to a specific dollar-per-hour usage rate for a one- or three-year term and then receive discounted rates for any usage that meets or exceeds the commitment. This allows users to save costs on a wide range of AWS services, including EC2, Fargate, Lambda, and more.&lt;/p>
&lt;p>Reserved Instances (RI) offer up to 75% savings compared to on-demand pricing for EC2 instances, RDS instances, and other services. Users can reserve capacity for a one or three-year term, and then receive discounted rates for the instances that match the reservation attributes. This allows users to save costs on predictable, steady-state workloads that run consistently over time.&lt;/p>
&lt;p>Depending on your workload and how your workload scales will ultimately be the driving force behind the decision to use either saving plans or reserved instances.&lt;/p>
&lt;h2 id="effectively-use-ecs-capacity-providers">Effectively use ECS Capacity Providers&lt;/h2>
&lt;p>ECS Capacity Providers allow users to define and manage groups of EC2 instances that can be used to run ECS tasks, with automatic scaling based on resource utilization and availability.&lt;/p>
&lt;p>Using Spot Instances as a scaling mechanism in ECS can further optimize costs and improve workload efficiency. Spot Instances are unused EC2 instances that can be rented at a significant discount compared to on-demand pricing. By using Spot Instances with ECS Capacity Providers, we can take advantage of these discounts while maintaining the desired availability and performance level.&lt;/p>
&lt;p>ECS can automatically manage the allocation of Spot Instances based on resource availability, helping to maximize cost savings while minimizing disruption to the workload.&lt;/p>
&lt;h2 id="switch-to-graviton-based-compute-instances">Switch to Graviton-based compute instances&lt;/h2>
&lt;p>In both EC2 and Lambda, we can switch over to Graviton based compute instances. They offer several benefits, including improved performance, cost efficiency, and reduced carbon footprint. Graviton is a custom-designed ARM-based processor optimized for AWS workloads, providing a high-performance, energy-efficient alternative to traditional x86-based instances.&lt;/p>
&lt;p>Your applications must be compatible with an ARM-based processor to take advantage of this, but you could receive up to 40%* savings depending on your workload.&lt;/p>
&lt;p>*From publically available sources&lt;/p>
&lt;h2 id="centralising-egress">Centralising Egress&lt;/h2>
&lt;p>One lesser-known tip is to centralise your egress. This involves creating a shared VPC that contains your NAT gateways and VPC Endpoints. These are two common costs in larger infrastructures that have many VPCs. There is a threshold that you’ll need to breach before this approach delivers you cost savings, which is a combination of the following:&lt;/p>
&lt;ol>
&lt;li>How many NAT gateways do you have?&lt;/li>
&lt;li>How many VPC endpoints do you use in each VPC?&lt;/li>
&lt;/ol>
&lt;h2 id="reduce-log-ingestion">Reduce Log Ingestion&lt;/h2>
&lt;p>The last tip concerns log ingestion. If you’re using AWS Cloudwatch to receive your logs, you might be paying too much for log ingestion. I’ve seen two common mistakes that lead to an increased cost:&lt;/p>
&lt;ol>
&lt;li>Duplicated logging. Teams may log directly to the Cloudwatch API, not realising that you already have ECS/Lambda capturing your logs.&lt;/li>
&lt;li>Logging too much: Teams may accidentally leave the log level set to verbose after diagnosing an issue, resulting in you ingesting much more than is necessary.&lt;/li>
&lt;/ol>
&lt;p>Both of these could easily increase your costs by 100s of dollars per month (if not more when talking about multiple environments), but it’s often hidden by other costs such as lack of EC2 reserved instances.&lt;/p>
&lt;p>Also, set a retention policy or backup your logs to S3 storage for even more savings.&lt;/p>
&lt;p>This only begins to scratch the surface of AWS cost savings. In fact, entire companies are dedicated to saving people money on AWS. Let me know your cost-saving tips below!&lt;/p>
&lt;p>&lt;strong>📣 Get the Weekly Newsletter Straight to Your Inbox! 📣&lt;/strong>&lt;/p>
&lt;p>Don&amp;rsquo;t miss out on the latest updates! Subscribe to the &lt;a href="https://news.codewithstu.tv">Observed! Newsletter&lt;/a> now and stay up-to-date with the latest tips and tricks across AWS, Devops and Architecture. &lt;a href="https://news.codewithstu.tv">Click here&lt;/a> to subscribe and start receiving your weekly dose of tech news!&lt;/p></description></item><item><title>Observed No. 10 - Chaos Engineering on AWS</title><link>https://im5tu.io/article/2023/03/observed-no.-10-chaos-engineering-on-aws/</link><pubDate>Mon, 20 Mar 2023 01:00:00 +0000</pubDate><guid>https://im5tu.io/article/2023/03/observed-no.-10-chaos-engineering-on-aws/</guid><description>&lt;p>Welcome to the 10th edition of Observed! Your weekly newsletter, where I bring you a tip you can implement in your infrastructure across many categories like AWS, Terraform and General DevOps practices. This week&amp;rsquo;s edition looks at the practice of Chaos Engineering.&lt;/p>
&lt;h2 id="what-is-chaos-engineering">What is Chaos Engineering?&lt;/h2>
&lt;p>Chaos engineering is an innovative approach to testing and enhancing complex systems&amp;rsquo; reliability, resilience, and robustness. Born out of a need to ensure system stability in the face of unpredictable events, chaos engineering involves intentionally injecting faults, errors, and failures into a system to evaluate its behaviour and improve its ability to withstand such occurrences.&lt;/p>
&lt;h2 id="origins">Origins&lt;/h2>
&lt;p>The concept of chaos engineering originated at Netflix in the early 2010s. Netflix recognized the need to ensure the reliability of its services in the face of ever-increasing traffic and infrastructure complexity. They understood that traditional testing methods were insufficient for detecting and addressing potential issues in their intricate systems.&lt;/p>
&lt;p>To tackle this challenge, Netflix engineers developed the Chaos Monkey, the first tool in what would become the Simian Army. The Chaos Monkey was designed to randomly disable instances (virtual machines) within Netflix&amp;rsquo;s production environment, forcing the system to adapt and recover from these disruptions. This approach allowed engineers to observe the system&amp;rsquo;s behaviour under stress and identify weaknesses that could lead to outages or performance degradation. As a result, Netflix continuously improved their infrastructure and services, enhancing user experience and customer satisfaction.&lt;/p>
&lt;p>Over time, chaos engineering has evolved into a comprehensive discipline with principles and practices that extend beyond the Netflix ecosystem. Many organizations have adopted chaos engineering to test and improve their systems, ensuring they can withstand the unexpected and function smoothly in the face of adversity.&lt;/p>
&lt;h2 id="why-should-we-adopt-chaos-engineering">Why should we adopt chaos engineering?&lt;/h2>
&lt;p>Adopting chaos engineering offers several benefits that can improve the overall reliability, resilience, and performance of your systems, including:&lt;/p>
&lt;ol>
&lt;li>Proactive problem identification: Discover and address potential issues in your systems before they escalate into more significant problems or outages by intentionally injecting faults.&lt;/li>
&lt;li>Improved system resilience: Regularly conducting chaos engineering experiments help build more resilient systems that can withstand and recover from disruptions, such as hardware failures, software bugs, or spikes in traffic.&lt;/li>
&lt;li>Faster incident response: Develop better processes and practices by routinely dealing with simulated failures. Teams become more adept at identifying, diagnosing, and resolving issues, ultimately reducing the time it takes to recover from incidents.&lt;/li>
&lt;li>Enhanced understanding of system behaviour: Gain insights into how your systems behave under various conditions. This understanding can help you optimize your infrastructure, fine-tune performance, and improve resource allocation, resulting in a more efficient and cost-effective system.&lt;/li>
&lt;/ol>
&lt;h2 id="how-do-we-apply-this-in-aws">How do we apply this in AWS?&lt;/h2>
&lt;p>AWS offers the Fault Injection Simulator (FIS) as a managed service to help you implement chaos engineering principles in your infrastructure. FIS allows you to inject faults into your AWS resources and observe their behaviour, enabling you to identify and address potential issues that could affect the resilience of your applications. AWS FIS contains:&lt;/p>
&lt;ol>
&lt;li>Experiment Templates: These pre-configured templates define the fault injection actions and their target AWS resources. You can create custom templates or use the ones provided by AWS.&lt;/li>
&lt;li>Experiments: An experiment is an instance of an experiment template that runs in your environment. It consists of one or more actions that inject faults into your AWS resources.&lt;/li>
&lt;li>Actions: Actions are the specific fault injection tasks during an experiment. Examples include terminating instances, injecting latency, or throttling APIs.&lt;/li>
&lt;li>Stop Conditions: These are criteria that, when met, automatically halt an experiment. They help ensure the safety of your environment by preventing experiments from causing excessive damage or disruption.&lt;/li>
&lt;/ol>
&lt;p>To use AWS FIS, we need to follow a few steps:&lt;/p>
&lt;ol>
&lt;li>Define the scope of your experiment: Identify the AWS resources and services you want to target for fault injection. Consider the potential impact on your environment and ensure you have the necessary safeguards, such as backup systems and monitoring tools.&lt;/li>
&lt;li>Create an experiment template: Using the FIS console or API, create an experiment template that specifies the actions you want to perform and the resources they will target. You can use AWS-provided templates or create custom ones based on your requirements.&lt;/li>
&lt;li>Set up stop conditions: Define the criteria that will trigger the automatic termination of your experiment. For example, you can set a stop condition based on the duration of the experiment, the number of errors encountered, or a specific metric value.&lt;/li>
&lt;li>Run the experiment: Launch the experiment using the FIS console or API. Monitor the progress of the experiment in real-time using AWS monitoring tools such as Amazon CloudWatch or AWS X-Ray.&lt;/li>
&lt;li>Analyze the results: After completing the experiment, review the results to identify any weaknesses in your infrastructure or application. Use this information to develop and implement improvements that will enhance the resilience of your system.&lt;/li>
&lt;li>Iterate and refine: Chaos engineering is an ongoing process. Continuously run experiments with different fault injection scenarios to ensure your system remains resilient under various conditions.&lt;/li>
&lt;/ol>
&lt;p>As technology evolves rapidly, our reliance on distributed systems and related services has grown significantly. Chaos engineering emerges as a crucial practice, helping organizations ensure that their systems can adapt and recover from unforeseen challenges. I believe chaos engineering represents a paradigm shift in how we approach complex systems&amp;rsquo; reliability and resilience.&lt;/p>
&lt;p>By embracing a continuous learning and improvement culture, teams can better understand their systems, enhancing their ability to respond to incidents and deliver a consistent, high-quality user experience.&lt;/p>
&lt;p>Furthermore, the importance of chaos engineering is magnified by the potential consequences of system failures. Downtime and performance issues can have severe financial, operational, and reputational impacts on organizations. Businesses can mitigate these risks by investing in chaos engineering and ultimately protecting their bottom line.&lt;/p>
&lt;p>&lt;strong>📣 Get the Weekly Newsletter Straight to Your Inbox! 📣&lt;/strong>&lt;/p>
&lt;p>Don&amp;rsquo;t miss out on the latest updates! Subscribe to the &lt;a href="https://news.codewithstu.tv">Observed! Newsletter&lt;/a> now and stay up-to-date with the latest tips and tricks across AWS, Devops and Architecture. &lt;a href="https://news.codewithstu.tv">Click here&lt;/a> to subscribe and start receiving your weekly dose of tech news!&lt;/p></description></item><item><title>Observed No. 9 - SLIs vs SLOs vs SLAs</title><link>https://im5tu.io/article/2023/03/observed-no.-9-slis-vs-slos-vs-slas/</link><pubDate>Mon, 13 Mar 2023 01:00:00 +0000</pubDate><guid>https://im5tu.io/article/2023/03/observed-no.-9-slis-vs-slos-vs-slas/</guid><description>&lt;p>Welcome to the 9th edition of Observed! Your weekly newsletter, where I bring you a tip you can implement in your infrastructure across many categories like AWS, Terraform and General DevOps practices. This week&amp;rsquo;s edition examines the differences between SLIs, SLOs and SLAs.&lt;/p>
&lt;p>When it comes to measuring the quality of your service, three terms are frequently used: Service Level Indicators (SLIs), Service Level Objectives (SLOs), and Service Level Agreements (SLAs). Although they sound similar, they each have different meanings and purposes. Let&amp;rsquo;s dive into each of them.&lt;/p>
&lt;h2 id="service-level-indicators-slis">Service Level Indicators (SLIs)&lt;/h2>
&lt;p>A Service Level Indicator (SLI) is a metric that measures the performance of a service. SLIs are used to understand a service&amp;rsquo;s performance from the end-users perspective. They are often measured in terms of availability, latency, and throughput.&lt;/p>
&lt;p>For example, with a website, you might use the following SLIs:&lt;/p>
&lt;ul>
&lt;li>Availability: The percentage of time that your website is up and running.&lt;/li>
&lt;li>Latency: The time it takes for your website to respond to a request.&lt;/li>
&lt;li>Throughput: The number of requests your website can handle at a time.&lt;/li>
&lt;/ul>
&lt;p>SLIs are generated on a per-event basis, such as a web request. Each event may feed into multiple SLIs and will create a result that must be one of the following:&lt;/p>
&lt;ul>
&lt;li>Passed: - We achieved our SLI for this event&lt;/li>
&lt;li>Failed: - We did not achieve our SLI for this event&lt;/li>
&lt;li>Not Interested: We are not interested in counting this event towards our SLI&lt;/li>
&lt;/ul>
&lt;p>Let&amp;rsquo;s look at the example of a web request and see how we can map SLIs to a web request event. Imagine that you want to have the following SLIs:&lt;/p>
&lt;ul>
&lt;li>Error rate&lt;/li>
&lt;li>Response time&lt;/li>
&lt;/ul>
&lt;p>For the web request, we could consider any 2XX responses as a success, 5XX responses as an error, and everything else we aren&amp;rsquo;t interested in (e.g., redirects). We may also consider ignoring specific endpoints such as health checks. We can apply the same logic to the response time SLI. We are generally only interested in the 2XX responses, so everything else is mapped to &amp;ldquo;not interested&amp;rdquo;. This would be generated from the same request/response data for the SLI error rate.&lt;/p>
&lt;h2 id="service-level-objectives-slos">Service Level Objectives (SLOs)&lt;/h2>
&lt;p>A Service Level Objective (SLO) is a target that defines an SLI&amp;rsquo;s acceptable performance level. SLOs are used to set expectations for how well a service should perform. SLOs are typically expressed as a percentage over a given period.&lt;/p>
&lt;p>For example, if your website has an SLI of availability, you might set an SLO of 99.9% over a month. This means your website should be available 99.9% of the time in any given month.&lt;/p>
&lt;p>What makes a good SLO?&lt;/p>
&lt;p>SLOs must be:&lt;/p>
&lt;ul>
&lt;li>Succinct&lt;/li>
&lt;li>Comprehensible&lt;/li>
&lt;li>Within our control (i.e., does not rely on user-specific actions such as # of created orders)&lt;/li>
&lt;li>Time-bound&lt;/li>
&lt;li>Specific&lt;/li>
&lt;/ul>
&lt;p>Some examples of good SLOs:&lt;/p>
&lt;ul>
&lt;li>Less than 1% of failed requests in the last 30 days&lt;/li>
&lt;li>99.9% Availability Per Month P95&lt;/li>
&lt;li>API Response time less than 500ms&lt;/li>
&lt;/ul>
&lt;h3 id="slo-adherence">SLO Adherence&lt;/h3>
&lt;p>The adherence to an SLO is always expressed as a percentage and only ever accounts for SLI events that interest us, e.g., Passed/Failed. We can think about SLOs using the following formula:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>SLO Adherence &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">100&lt;/span> * &lt;span style="color:#f92672">(&lt;/span>passed / &lt;span style="color:#f92672">(&lt;/span>passed + failed&lt;span style="color:#f92672">))&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If we have 132 events that we are interested in, 5 of which failed, then the calculation would be as follows:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>Passed &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">127&lt;/span> events
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Failed &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">5&lt;/span> events
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>SLO Adherence &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">100&lt;/span> * &lt;span style="color:#f92672">(&lt;/span>&lt;span style="color:#ae81ff">127&lt;/span> / &lt;span style="color:#f92672">(&lt;/span>&lt;span style="color:#ae81ff">127&lt;/span> + 5&lt;span style="color:#f92672">))&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>SLO Adherence &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">100&lt;/span> * &lt;span style="color:#f92672">(&lt;/span>&lt;span style="color:#ae81ff">127&lt;/span> / 132&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>SLO Adherence &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">100&lt;/span> * 0.9621212121212121
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>SLO Adherence &lt;span style="color:#f92672">=&lt;/span> 96.21% &lt;span style="color:#f92672">(&lt;/span>rounded to 2dp&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Each SLO we publish should be available on a continually updated basis.&lt;/p>
&lt;h2 id="service-level-agreements-slas">Service Level Agreements (SLAs)&lt;/h2>
&lt;p>A Service Level Agreement (SLA) is a contract between a service provider and a customer that defines the level of service the provider will deliver. SLAs are used to establish a mutual understanding between the provider and the customer regarding the level of service that will be provided.&lt;/p>
&lt;p>For example, a cloud provider might offer an SLA guaranteeing 99.9% availability for your cloud services. If you fail to meet this SLA, you may have to provide a service credit or refund to the customer.&lt;/p>
&lt;p>In conclusion, SLIs, SLOs, and SLAs are all critical components of measuring the quality of your service. SLIs measure the performance, SLOs set the acceptable level of performance, and SLAs establish the level of service that will be delivered. You can ensure that your service meets your customers&amp;rsquo; needs by tracking and meeting these metrics.&lt;/p>
&lt;p>&lt;strong>📣 Get the Weekly Newsletter Straight to Your Inbox! 📣&lt;/strong>&lt;/p>
&lt;p>Don&amp;rsquo;t miss out on the latest updates! Subscribe to the &lt;a href="https://news.codewithstu.tv">Observed! Newsletter&lt;/a> now and stay up-to-date with the latest tips and tricks across AWS, Devops and Architecture. &lt;a href="https://news.codewithstu.tv">Click here&lt;/a> to subscribe and start receiving your weekly dose of tech news!&lt;/p></description></item><item><title>Observed No. 8 - Continuous Integration vs Continuous Delivery vs Continuous Deployment</title><link>https://im5tu.io/article/2023/03/observed-no.-8-continuous-integration-vs-continuous-delivery-vs-continuous-deployment/</link><pubDate>Mon, 06 Mar 2023 00:00:00 +0000</pubDate><guid>https://im5tu.io/article/2023/03/observed-no.-8-continuous-integration-vs-continuous-delivery-vs-continuous-deployment/</guid><description>&lt;p>Welcome to the 8th edition of Observed! Your weekly newsletter, where I bring you a tip you can implement in your infrastructure across many categories like AWS, Terraform and General DevOps practices. This week&amp;rsquo;s edition looks at the differences between continuous integration, continuous delivery and continuous deployment.&lt;/p>
&lt;h2 id="continuous-integration">Continuous Integration&lt;/h2>
&lt;p>Continuous Integration (CI) is the act of integrating code changes into a shared repository, and automated tests verify each integration. The aim is to catch and fix errors early in the development cycle rather than waiting for a significant release.&lt;/p>
&lt;p>With CI, developers frequently work on small, incremental changes and commit them to the main codebase. When developers push their code changes to the repository, the CI system automatically builds the code. It runs automated tests to ensure the changes don&amp;rsquo;t break anything in the codebase. If the tests fail, the system alerts the developer, who can quickly fix the issue before it causes any problems.&lt;/p>
&lt;p>CI helps to reduce the risk of integration conflicts and reduces the time required to test and integrate new code changes. Ultimately, CI leads to a more stable and reliable software development process.&lt;/p>
&lt;h2 id="continuous-delivery">Continuous Delivery&lt;/h2>
&lt;p>Continuous delivery is a natural extension of continuous integration where each change gets released to a staging or test environment in a reliable and automated way. With CD, your software is always in a releasable state, and the decision to release becomes based on business needs rather than technical constraints.&lt;/p>
&lt;p>Continuous delivery doesn&amp;rsquo;t necessarily mean that each change makes its way to production, simple that each change could make it to production.&lt;/p>
&lt;h2 id="continuous-deployment">Continuous Deployment&lt;/h2>
&lt;p>Continuous deployment is very similar to, and often confused with, continuous delivery. Continuous deployment releases code changes to production after they pass automated testing. With continuous deployment, developers can deploy new code changes to production without manual intervention.&lt;/p>
&lt;p>Continuous deployment relies heavily on automation and infrastructure stability. This practice helps to improve the speed of software delivery, reduce the risk of human error, and ensure that new features and updates are available to users as soon as possible. It helps reduce the time required to release new features and updates, as no manual intervention is necessary to deploy changes to production.&lt;/p>
&lt;p>It is important to note that continuous deployment requires a high level of trust in the automated testing process and infrastructure stability. Any issues with the automated testing or deployment process could lead to downtime or other issues in production.&lt;/p>
&lt;h2 id="looking-at-the-benefits-of-cicdcd">Looking at the benefits of CI/CD/CD&lt;/h2>
&lt;p>By understanding the differences between CI/CD/CD, you can implement the practices to improve your software development process and meet your business requirements. Some of the key benefits include:&lt;/p>
&lt;p>Smoother &amp;amp; More Frequent Releases: Automating with a continuous deployment pipeline means releases can happen with the push of a button rather than taking days to plan for and execute. Customers also stay up-to-date with the latest version of the software.&lt;/p>
&lt;p>Safer Releases: Since automated processes like continuous delivery and deployment work in smaller change batches, issues in each release are easier to remediate than the code changes pushed out with infrequent, mammoth-sized manual releases. Smaller releases mean that we also reduce the risk of each change.&lt;/p>
&lt;p>Less Manual Work: Overall, less time is spent releasing, which means that more time can be spent increasing the quality of your software.&lt;/p>
&lt;p>&lt;strong>📣 Get the Weekly Newsletter Straight to Your Inbox! 📣&lt;/strong>&lt;/p>
&lt;p>Don&amp;rsquo;t miss out on the latest updates! Subscribe to the &lt;a href="https://news.codewithstu.tv">Observed! Newsletter&lt;/a> now and stay up-to-date with the latest tips and tricks across AWS, Devops and Architecture. &lt;a href="https://news.codewithstu.tv">Click here&lt;/a> to subscribe and start receiving your weekly dose of tech news!&lt;/p></description></item><item><title>Observed No. 7 - Well Architected</title><link>https://im5tu.io/article/2023/02/observed-no.-7-well-architected/</link><pubDate>Mon, 27 Feb 2023 00:00:00 +0000</pubDate><guid>https://im5tu.io/article/2023/02/observed-no.-7-well-architected/</guid><description>&lt;p>Welcome to the seventh edition of Observed! Your weekly newsletter, where I bring you a tip you can implement in your infrastructure across many categories like AWS, Terraform and General DevOps practices. This week&amp;rsquo;s edition looks at the Well-Architected framework.&lt;/p>
&lt;h2 id="what-is-the-well-architected-framework">What is the Well-Architected Framework?&lt;/h2>
&lt;p>The Well-Architected Framework is a set of best practices and guidelines designed to help businesses build and operate reliable, secure, efficient, and cost-effective systems in the cloud. AWS conceived the framework to help customers evaluate their architecture and adopt best practices to improve their systems&amp;rsquo; performance, security, and scalability.&lt;/p>
&lt;p>The framework has six pillars, each focusing on a specific aspect of running on AWS. These are:&lt;/p>
&lt;ol>
&lt;li>Operational Excellence Pillar&lt;/li>
&lt;li>Security Pillar&lt;/li>
&lt;li>Reliability Pillar&lt;/li>
&lt;li>Performance Efficiency Pillar&lt;/li>
&lt;li>Cost Optimization Pillar&lt;/li>
&lt;li>Sustainability Pillar&lt;/li>
&lt;/ol>
&lt;h2 id="operational-excellence-pillar">Operational Excellence Pillar&lt;/h2>
&lt;p>The operational excellence pillar focuses on improving operating procedures and processes, monitoring systems, and continuously improving the overall operational capabilities of the organization. It provides best practices for managing change, responding to events and defining procedures to ensure consistent, repeatable processes are in place.&lt;/p>
&lt;h2 id="security-pillar">Security Pillar&lt;/h2>
&lt;p>The security pillar provides best practices for identifying and managing security risks, such as implementing strong access controls and enforcing least privilege principles. It also emphasizes the importance of automation of security tasks, continuous monitoring for security threats and maintaining compliance with security standards and regulations.&lt;/p>
&lt;h2 id="reliability-pillar">Reliability Pillar&lt;/h2>
&lt;p>The reliability pillar provides best practices for designing resilient systems, such as using distributed systems and redundancy to ensure high availability and implementing monitoring and alerting to quickly detect and respond to failures. It also emphasizes the importance of testing and validating system resilience to identify and address potential weaknesses before they impact users.&lt;/p>
&lt;h2 id="performance-efficiency-pillar">Performance Efficiency Pillar&lt;/h2>
&lt;p>The performance efficiency pillar provides best practices for selecting suitable instance types and sizes, using automation to scale resources up and down to meet demand, and optimizing application performance by leveraging caching, database performance tuning, and content delivery networks. It also emphasizes the importance of monitoring performance and usage metrics to identify areas for optimization and improvement.&lt;/p>
&lt;h2 id="cost-optimization-pillar">Cost Optimization Pillar&lt;/h2>
&lt;p>The cost optimization pillar provides best practices for selecting suitable pricing models, monitoring and analyzing usage data to identify opportunities for cost optimization, and implementing mechanisms for cost control, such as automated resource scheduling and usage quotas. It also emphasizes the importance of designing architectures that can scale cost-effectively by leveraging cloud services that offer pay-as-you-go pricing and dynamic resource allocation. By following the guidance of this pillar, organizations can optimize their cloud spending, reduce unnecessary costs, and maximize the value they get from their cloud investments.&lt;/p>
&lt;h2 id="sustainability-pillar">Sustainability Pillar&lt;/h2>
&lt;p>The sustainability pillar is the latest addition to the framework and focuses on designing and operating sustainable systems in the cloud. AWS introduced this pillar to minimize IT systems&amp;rsquo; environmental impact whilst reducing costs and downstream impacts.&lt;/p>
&lt;h2 id="are-you-well-architected">Are you well-architected?&lt;/h2>
&lt;p>You can assess your adherence to the AWS well-architected framework in two ways. The first option is to use an external consultancy company, which AWS can recommend partners for you. Typically these engagements are free to carry out the review but often come with an expectation that the consultancy would carry out some remediation work for you as a paid service. Some AWS partners may offer AWS credits for conducting the review.&lt;/p>
&lt;p>The second option is to run the assessment yourself in the AWS console, which is entirely free, using the AWS Well-Architected Tool. They have three different lenses at the time of writing:&lt;/p>
&lt;p>AWS Well-Architected Framework: The AWS Well-Architected Framework Lens provides foundational questions for you to consider for all your cloud architectures.&lt;/p>
&lt;p>Serverless Lens: The AWS Serverless Application Lens provides additional questions for you to consider for your serverless applications.&lt;/p>
&lt;p>SaaS Lens: The AWS SaaS Lens provides additional questions for you to consider for your Software-as-a-Service (SaaS) applications.&lt;/p>
&lt;p>AWS recommends enabling Trusted Advisor when you start the tool if you have access, as this will provide more context to your questions. The questions are relatively straightforward, but I&amp;rsquo;d recommend talking with your AWS account manager to see if they can provide some training for you and help you walk through the first one.&lt;/p>
&lt;p>You can run through the well-architected framework question in the AWS console using the AWS Well-Architected Tool: &lt;a href="https://eu-west-1.console.aws.amazon.com/wellarchitected/home">https://eu-west-1.console.aws.amazon.com/wellarchitected/home&lt;/a>&lt;/p>
&lt;p>Learn more about the Well-Architected framework here: &lt;a href="https://aws.amazon.com/architecture/well-architected">https://aws.amazon.com/architecture/well-architected&lt;/a>&lt;/p>
&lt;p>&lt;strong>📣 Get the Weekly Newsletter Straight to Your Inbox! 📣&lt;/strong>&lt;/p>
&lt;p>Don&amp;rsquo;t miss out on the latest updates! Subscribe to the &lt;a href="https://news.codewithstu.tv">Observed! Newsletter&lt;/a> now and stay up-to-date with the latest tips and tricks across AWS, Devops and Architecture. &lt;a href="https://news.codewithstu.tv">Click here&lt;/a> to subscribe and start receiving your weekly dose of tech news!&lt;/p></description></item><item><title>Unlocking the best of AWS Route 53</title><link>https://im5tu.io/video/unlocking-the-best-of-aws-route-53/</link><pubDate>Sun, 26 Feb 2023 07:47:00 +0000</pubDate><guid>https://im5tu.io/video/unlocking-the-best-of-aws-route-53/</guid><description>&lt;p>In this video, I will show you the power of wildcards, health checks, and, my favourite, a Netflix-style multi-region DNS setup for scenarios on AWS Route 53. You&amp;rsquo;ll learn about the many capabilities of this service and how to use it to your advantage. Each section is accompanied by Terraform code. This video was originally was posted under DevOpsWithStu, but since then I have merged the channels together.&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube-nocookie.com/embed/oKyouRHsSVw" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;h2 id="video-links">Video Links&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://www.youtube.com/watch?v=WDDkLOT8SCk">Netflix: Multi-Regional Resiliency and Amazon Route 53&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>How to Use AWS IAM Identity Centre with Terraform</title><link>https://im5tu.io/video/how-to-use-aws-iam-identity-centre-with-terraform/</link><pubDate>Sun, 26 Feb 2023 07:46:01 +0000</pubDate><guid>https://im5tu.io/video/how-to-use-aws-iam-identity-centre-with-terraform/</guid><description>&lt;p>This video will look at how to log in with AWS IAM Identity Centre and what to do when Terraform doesn&amp;rsquo;t work out of the box! Learn some of the inner mechanics behind AWS SSO Login. This video was originally was posted under DevOpsWithStu, but since then I have merged the channels together.&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube-nocookie.com/embed/CfA-pOQK8Fg" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div></description></item><item><title>Create Terraform Modules Like A Pro</title><link>https://im5tu.io/video/create-terraform-modules-like-a-pro/</link><pubDate>Sun, 26 Feb 2023 07:45:00 +0000</pubDate><guid>https://im5tu.io/video/create-terraform-modules-like-a-pro/</guid><description>&lt;p>In this video, we&amp;rsquo;ll look at a technique you can use to upgrade your Terraform modules. Over the course of the last 4/5 years, I&amp;rsquo;ve noticed that there is always a trend within companies to build modules for specific things/use cases. Rarely do I see these teams account for the one thing they need to operate the infrastructure they make. This video was originally was posted under DevOpsWithStu, but since then I have merged the channels together.&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube-nocookie.com/embed/UvuFWued8_M" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div></description></item><item><title>Observed No. 6 - Service Meshes</title><link>https://im5tu.io/article/2023/02/observed-no.-6-service-meshes/</link><pubDate>Mon, 20 Feb 2023 02:00:00 +0000</pubDate><guid>https://im5tu.io/article/2023/02/observed-no.-6-service-meshes/</guid><description>&lt;p>Welcome to the sixth edition of Observed! Your weekly newsletter, where I bring you a tip you can implement in your infrastructure across many categories like AWS, Terraform and General DevOps practices. This week&amp;rsquo;s edition looks at service meshes.&lt;/p>
&lt;h2 id="what-is-a-service-mesh">What is a service mesh?&lt;/h2>
&lt;p>A service mesh is dedicated infrastructure for managing service-to-service communication within a microservices architecture. It provides a way to manage the complex network of microservices by adding a layer of abstraction between the services and the underlying network.&lt;/p>
&lt;p>In a service mesh architecture, each service instance has a sidecar proxy that manages the communication between services. The proxies handle low-level network traffic, including load balancing, service discovery, traffic routing, and security.&lt;/p>
&lt;p>The service mesh provides a centralised platform for managing and monitoring the communication between services. It provides a way to configure and manage the communication between services without the need to modify the services themselves. The service mesh can also offer advanced features such as circuit breaking, rate limiting, and observability that can help improve the reliability and performance of the microservices architecture.&lt;/p>
&lt;h2 id="what-are-the-key-benefits-of-a-service-mesh">What are the key benefits of a service mesh?&lt;/h2>
&lt;ol>
&lt;li>Traffic management and load balancing: Service meshes provide a way to automatically route traffic between services, distribute load, and implement traffic shaping strategies, such as canary deployments, blue/green deployments, and A/B testing.&lt;/li>
&lt;li>Service discovery: Service meshes provide a centralised platform for discovering and managing the network of microservices. The mesh can automatically detect new services as they deploy and provide a way to route traffic to those services.&lt;/li>
&lt;li>Observability and tracing: Service meshes provide a way to monitor and trace traffic flow between services, which can help identify performance bottlenecks and troubleshoot issues.&lt;/li>
&lt;li>Security: Service meshes can provide security features such as mutual TLS, authentication, and authorisation to ensure that communication between services is secure and encrypted.&lt;/li>
&lt;/ol>
&lt;p>Simplified deployment and management: Service meshes provide a way to manage the network of microservices in a centralised platform, which can simplify deployment, configuration, and management of the microservices architecture, reducing the complexity of managing many services and ensuring that the architecture is consistent and reliable.&lt;/p>
&lt;h2 id="implementations-of-a-service-mesh">Implementations of a service mesh&lt;/h2>
&lt;p>There are many different products on the market, most of which are open source, that provide part or all of the features described above for a service mesh. The most common ones include the following:&lt;/p>
&lt;ol>
&lt;li>Istio: Istio is an open-source service mesh platform that provides traffic management, security, and observability features. It is designed to be vendor-neutral and integrates with Kubernetes, Docker, and other container orchestrators.&lt;/li>
&lt;li>Linkerd: Linkerd is an ultralight service mesh for Kubernetes and other cloud-native environments. It provides features such as traffic management, service discovery, and observability which is designed to be easy to deploy and manage.&lt;/li>
&lt;li>Consul: Consul is a service mesh platform from HashiCorp that provides service discovery, configuration, and segmentation. It can also provide traffic management and security features designed to work with multiple deployment environments, including Kubernetes, VMs, and bare metal.&lt;/li>
&lt;li>AWS App Mesh: AWS App Mesh is a service mesh platform that provides traffic management, observability, and security features for applications deployed on Amazon Web Services (AWS). It supports both containerised and non-containerised applications and can be integrated with other AWS services.&lt;/li>
&lt;/ol>
&lt;h2 id="do-i-need-a-service-mesh">Do I need a service mesh?&lt;/h2>
&lt;p>Service meshes are most beneficial when you have a decent amount of services in your microservices architecture. However, implementing a service mesh can add complexity and overhead and may not be necessary for smaller architectures. If you only have a small number of services, you can manage them using more straightforward tools and techniques. The exact threshold on when to implement a service mesh will depend on the specific use case.&lt;/p>
&lt;p>&lt;strong>📣 Get the Weekly Newsletter Straight to Your Inbox! 📣&lt;/strong>&lt;/p>
&lt;p>Don&amp;rsquo;t miss out on the latest updates! Subscribe to the &lt;a href="https://news.codewithstu.tv">Observed! Newsletter&lt;/a> now and stay up-to-date with the latest tips and tricks across AWS, Devops and Architecture. &lt;a href="https://news.codewithstu.tv">Click here&lt;/a> to subscribe and start receiving your weekly dose of tech news!&lt;/p></description></item><item><title>Observed No. 5 - MACH Architectures</title><link>https://im5tu.io/article/2023/01/observed-no.-5-mach-architectures/</link><pubDate>Mon, 30 Jan 2023 02:00:00 +0000</pubDate><guid>https://im5tu.io/article/2023/01/observed-no.-5-mach-architectures/</guid><description>&lt;p>Welcome to the fifth edition of Observed! Your weekly newsletter, where I bring you a tip you can implement in your infrastructure across many categories like AWS, Terraform and General DevOps practices. This week&amp;rsquo;s edition looks at MACH architectures.&lt;/p>
&lt;h2 id="what-are-mach-architectures">What are MACH architectures?&lt;/h2>
&lt;p>A MACH architecture is a set of principles for modern application architectures. MACH is a relatively new term in the industry and is quickly gaining popularity because of the level of interoperability, scalability and composability. Many systems today are being built like lego pieces on cloud infrastructure which may be composed to create larger systems with well-defined boundaries thanks to movements like Domain Driven Design.&lt;/p>
&lt;p>The MACH acronym consists of four distinct parts:&lt;/p>
&lt;ul>
&lt;li>M is for Microservices: Individual pieces of business functionality that are independently developed, deployed and managed.&lt;/li>
&lt;li>A is for API-first: All functionality is available on the API.&lt;/li>
&lt;li>C is for Cloud-Native SaaS: SaaS that leverages the cloud beyond storage and hosting, including elastic scaling and automatic updating.&lt;/li>
&lt;li>H is for Headless: Front-end presentation is decoupled from back-end logic and channel, programming language, and is framework agnostic.&lt;/li>
&lt;/ul>
&lt;p>The above definitions come directly from the MACH alliance.&lt;/p>
&lt;h3 id="what-is-the-mach-alliance">What is the MACH Alliance?&lt;/h3>
&lt;p>The MACH Alliance is a not-for-profit industry body that advocates for open and best-of-breed enterprise technology ecosystems. The Alliance is a vendor-neutral institution that provides resources, education and guidance through industry experts to support companies on their journey.&lt;/p>
&lt;h2 id="what-are-the-benefits-of-mach-architectures">What are the benefits of MACH architectures?&lt;/h2>
&lt;p>There are many benefits of MACH architecture, including the following:&lt;/p>
&lt;ol>
&lt;li>Faster development with reduced risk: Quickly bring ideas to market with a quicker route to MVP by utilising independent microservices which don&amp;rsquo;t affect the rest of the architecture negatively.&lt;/li>
&lt;li>Best-of-breed technology: Utilise the best available technology whilst integrating existing functionality where it&amp;rsquo;s appropriate to do so.&lt;/li>
&lt;li>Reducing the need to upgrade: Automatic and non-breaking releases eliminate the worry of disruptive upgrades as they communicate through your APIs, creating an excellent level of separation.&lt;/li>
&lt;li>Easy customisation and innovation: Quickly adapt to changing customer needs with the ability to change and innovate the customer experience constantly.&lt;/li>
&lt;/ol>
&lt;h2 id="what-are-the-drawbacks-of-mach-architectures">What are the drawbacks of MACH Architectures?&lt;/h2>
&lt;p>When evaluating any architectural design, we must consider the impacts of our decision to ensure that it&amp;rsquo;s the right one. MACH-based architectures are no different, and it&amp;rsquo;s not all sunshine and rainbows, especially for smaller businesses:&lt;/p>
&lt;p>Microservice can be costly to develop and maintain, leading to a complex architecture. As more microservices are developed, additional technologies such as API gateways, service discovery, and service meshes are needed to manage them effectively.&lt;/p>
&lt;p>Ensuring consistency and a well-designed API surface takes a lot of skill, experience and maintenance. Any API would also need to consider how to version the API to ensure that clients do not break.&lt;/p>
&lt;p>On-premise deployments are still a problem, typically found in finance and government-related areas.&lt;/p>
&lt;p>Cost. Whenever we talk about utilising the cloud and expanding into many microservices, there is always an inherent cost. Some companies can deal with these costs, but purse strings are generally tightening a lot at the moment.&lt;/p>
&lt;h2 id="is-a-mach-architecture-right-for-you">Is a MACH architecture right for you?&lt;/h2>
&lt;p>Sitecore has compiled 11 great questions to ask before you consider transitioning into the MACH architecture strategy:&lt;/p>
&lt;ol>
&lt;li>Does it feature true microservices?&lt;/li>
&lt;li>Can you execute phased roll-outs?&lt;/li>
&lt;li>Does it support a best-of-breed approach?&lt;/li>
&lt;li>Is it built with APIs from the ground up, or has it adopted an API bolt-on strategy?&lt;/li>
&lt;li>Can you access quality documentation?&lt;/li>
&lt;li>How are integrations completed?&lt;/li>
&lt;li>Does it offer limitless scalability?&lt;/li>
&lt;li>Is the software delivered as-a-service (SaaS)?&lt;/li>
&lt;li>Do updates and upgrades happen via continuous delivery without breaking changes?&lt;/li>
&lt;li>Can you &amp;ldquo;see&amp;rdquo; the administrative or buyer interface without development time?&lt;/li>
&lt;li>Can you develop and deploy the user experience freely and flexibly?&lt;/li>
&lt;/ol>
&lt;p>My view on technology is that there is never a one size fits all approach, and MACH architectures are no different. From what I&amp;rsquo;ve seen, most companies are already ~75% of the way to a MACH architecture. It&amp;rsquo;s hard for anyone to realise the constraints of any given business from the outside, but these are some excellent principles to follow where we can.&lt;/p>
&lt;p>I would love to hear your thoughts on these principals.&lt;/p>
&lt;p>&lt;strong>📣 Get the Weekly Newsletter Straight to Your Inbox! 📣&lt;/strong>&lt;/p>
&lt;p>Don&amp;rsquo;t miss out on the latest updates! Subscribe to the &lt;a href="https://news.codewithstu.tv">Observed! Newsletter&lt;/a> now and stay up-to-date with the latest tips and tricks across AWS, Devops and Architecture. &lt;a href="https://news.codewithstu.tv">Click here&lt;/a> to subscribe and start receiving your weekly dose of tech news!&lt;/p></description></item><item><title>Observed No. 4 - Emerging Pattern: Centralised Ingress</title><link>https://im5tu.io/article/2023/01/observed-no.-4-emerging-pattern-centralised-ingress/</link><pubDate>Mon, 23 Jan 2023 02:00:00 +0000</pubDate><guid>https://im5tu.io/article/2023/01/observed-no.-4-emerging-pattern-centralised-ingress/</guid><description>&lt;p>Welcome to the fourth edition of Observed! Your weekly newsletter, where I bring you a tip you can implement in your infrastructure across many categories like AWS, Terraform and General DevOps practices. This week&amp;rsquo;s edition looks at a common pattern emerging across the industry: Centralised Ingress.&lt;/p>
&lt;h2 id="what-is-ingress-traffic">What is ingress traffic?&lt;/h2>
&lt;p>Ingress traffic refers to communication with your network from outside its perimeter. Typically when referring to ingress traffic, we talk about traffic from external consumers of our services, usually via HTTP or HTTPS. However, ingress could be any external traffic trying to hit our network. For example, it could be a Google search bot or an attacker trying to connect to our Redis cluster(s).&lt;/p>
&lt;h2 id="why-are-companies-centralising-ingress">Why are companies centralising ingress?&lt;/h2>
&lt;p>In the past, the companies implementing centralised ingress have been limited to large companies with tens of thousands of employees. As the technology improves and teams adopt more agile DevOps practices, companies as small as 50 people are implementing this pattern.&lt;/p>
&lt;p>To get a good understanding of why this is an emerging pattern, let’s take a look at some of the benefits that companies will get by implementing a centralised ingestion layer:&lt;/p>
&lt;ol>
&lt;li>Improved security: Directing all incoming traffic to a central point can be more easily monitored for security threats, and any malicious traffic can be blocked before it reaches the internal network. Centralisation also reduces the total attack surface by keeping everything private, that should be private.&lt;/li>
&lt;li>Simplified network architecture: Directing all incoming traffic to a central point can simplify the overall network architecture and make it easier to understand and troubleshoot. The simplification may also lead to cost savings by reducing the total number of load balancers, depending on the final architecture.&lt;/li>
&lt;li>Additional functionality: Using a centralised ingestion point as a reverse proxy can provide other functionality like SSL termination, caching, rate limiting, and a starting point for tracing or authentication.&lt;/li>
&lt;/ol>
&lt;p>From what I’ve seen, companies tend to move towards a centralised point of ingestion primarily for security benefits, closely followed by the additional functionality they receive.&lt;/p>
&lt;p>Companies typically look at two main additional pieces of functionality:&lt;/p>
&lt;ol>
&lt;li>Rate limiting&lt;/li>
&lt;li>Tracing&lt;/li>
&lt;/ol>
&lt;p>Centralising the rate-limiting of all external clients in a centralised manner allows development teams to reduce the total complexity of their applications because they essentially offload the work to the point of ingress. Teams may still choose to have rate limiting for their internal clients, but the centralised view can provide rate limits that are not otherwise possible to implement in each application.&lt;/p>
&lt;p>With Tracing, a centralised ingress is the starting point for all requests regardless of destination. Apart from the standard benefits of having a distributed tracing system, one key benefit of starting the tracing at a single entry point is that you can generate metrics for every endpoint in your system, including any associated monitoring and alerting.&lt;/p>
&lt;h2 id="why-wouldnt-you-centralise-your-ingress">Why wouldn’t you centralise your ingress?&lt;/h2>
&lt;p>Whilst there are a lot of positives of centralising your ingress traffic, there may be occasions where you shouldn’t. These include:&lt;/p>
&lt;ol>
&lt;li>Scaling: Centralising your ingress traffic can create a bottleneck if the point of ingestion cannot handle a large amount of incoming traffic. This can lead to increased latency and decreased performance, or in some cases, a complete denial of service.&lt;/li>
&lt;li>Complexity: Centralising your ingress traffic can add complexity to the architecture, making it more difficult to understand and troubleshoot. Moreover, it can increase the risk of any deployments done to the ingestion layer, which must be managed accordingly.&lt;/li>
&lt;li>Limited flexibility: Centralising your ingress traffic can limit how traffic is directed and managed. It may be harder to implement more advanced routing rules or to route traffic to different services based on certain conditions.&lt;/li>
&lt;/ol>
&lt;p>As with any technology, the benefits and drawbacks need to be reviewed by your organisation against any requirements that they have. When deploying a centralised ingress layer, you also need to consider how many you will need to deploy because, ideally, you would have at least two different ingestion layers—one for production and one for testing.&lt;/p>
&lt;p>If you want to see a video on deploying a centralised ingress network on AWS, please drop me a message or a comment.&lt;/p>
&lt;p>&lt;strong>📣 Get the Weekly Newsletter Straight to Your Inbox! 📣&lt;/strong>&lt;/p>
&lt;p>Don&amp;rsquo;t miss out on the latest updates! Subscribe to the &lt;a href="https://news.codewithstu.tv">Observed! Newsletter&lt;/a> now and stay up-to-date with the latest tips and tricks across AWS, Devops and Architecture. &lt;a href="https://news.codewithstu.tv">Click here&lt;/a> to subscribe and start receiving your weekly dose of tech news!&lt;/p></description></item><item><title>Observed No. 3 - Understanding Split Horizon DNS: How it works and How to Implement it in AWS</title><link>https://im5tu.io/article/2023/01/observed-no.-3-understanding-split-horizon-dns-how-it-works-and-how-to-implement-it-in-aws/</link><pubDate>Mon, 16 Jan 2023 00:00:00 +0000</pubDate><guid>https://im5tu.io/article/2023/01/observed-no.-3-understanding-split-horizon-dns-how-it-works-and-how-to-implement-it-in-aws/</guid><description>&lt;p>Welcome to the third edition of Observed! Your weekly newsletter, where I bring you a tip you can implement in your infrastructure across many categories like AWS, Terraform and General DevOps practices. This week&amp;rsquo;s edition looks at Split Horizon DNS.&lt;/p>
&lt;h2 id="what-is-split-horizon-dns">What is Split Horizon DNS?&lt;/h2>
&lt;p>Split Horizon DNS is a technique used in DNS to provide different responses to queries depending on where the query originates. For example, a DNS request originating from inside your network may elicit a different response to a DNS request from a consumer of your application.&lt;/p>
&lt;p>Splitting the responses by source can help ensure that only the resources which should be exposed to the internet are exposed. For example, an internal-only admin service would be an ideal candidate for not exposing to the internet, but we would want it addressable by our internal networks. In this case&lt;/p>
&lt;p>With Split Horizon DNS, each zone responds with an authoritative answer. For example, in a traditional DNS setup where the DNS is not split, there is only one authoritative answer - your primary DNS nameserver. With split DNS, your internal DNS will respond with one answer, and the external DNS will respond with another - typically for an internal and external load balancer, respectively.&lt;/p>
&lt;p>Split Horizon DNS is also known as Split View DNS, Split DNS or Split Brain DNS.&lt;/p>
&lt;h2 id="how-to-set-up-split-horizon-dns-in-aws">How to set up Split Horizon DNS in AWS?&lt;/h2>
&lt;p>To configure Split Horizon DNS, you perform the following steps:&lt;/p>
&lt;ol>
&lt;li>Create public and private hosted zones with the same name, for example: mydomain.com&lt;/li>
&lt;li>Associate one or more VPCs with the private hosted zone. Route 53 Resolver uses the private hosted zone to route DNS queries in the specified VPCs.&lt;/li>
&lt;li>Create records in each hosted zone. Records in the public-hosted zone control where internet traffic is routed, whilst records in the private-hosted zone control how traffic is routed internally.&lt;/li>
&lt;li>Query your DNS&lt;/li>
&lt;/ol>
&lt;p>If you have any questions or comments, please don’t hesitate to contact me either in the comments, on Twitter or any medium listed on my website! I’d love to hear your thoughts. Subscribe to the newsletter below!&lt;/p>
&lt;p>&lt;strong>📣 Get the Weekly Newsletter Straight to Your Inbox! 📣&lt;/strong>&lt;/p>
&lt;p>Don&amp;rsquo;t miss out on the latest updates! Subscribe to the &lt;a href="https://news.codewithstu.tv">Observed! Newsletter&lt;/a> now and stay up-to-date with the latest tips and tricks across AWS, Devops and Architecture. &lt;a href="https://news.codewithstu.tv">Click here&lt;/a> to subscribe and start receiving your weekly dose of tech news!&lt;/p></description></item><item><title>Observed No. 2 - Upgrade Your Terraform Modules</title><link>https://im5tu.io/article/2023/01/observed-no.-2-upgrade-your-terraform-modules/</link><pubDate>Mon, 09 Jan 2023 00:00:00 +0000</pubDate><guid>https://im5tu.io/article/2023/01/observed-no.-2-upgrade-your-terraform-modules/</guid><description>&lt;p>Welcome to the second edition of Observed! Your weekly newsletter, where I bring you a tip you can implement in your infrastructure across many categories like AWS, Terraform and General DevOps practices. This week&amp;rsquo;s edition looks at a technique you can use to upgrade your Terraform modules.&lt;/p>
&lt;p>In case you aren’t sure what a Terraform module is, they are a self-contained package of Terraform configurations managed as a group. Modules can be used to create reusable components, improve organization and structure, and improve the reusability and maintainability of your infrastructure.&lt;/p>
&lt;p>Over the course of the last 4/5 years, I’ve noticed that there is always a trend within companies to build modules for specific things/use cases. Rarely do I see these teams account for the one thing they need to operate the infrastructure they make. That is monitoring/alerting.&lt;/p>
&lt;p>Typically, a team would use an existing module or create their own and then create a load of resources for their monitoring/alerting needs. They would re-write this same terraform code over and over.&lt;/p>
&lt;p>There are two different approaches that I take with my teams when adding monitoring/alerting:&lt;/p>
&lt;p>Create a separate module which contains all of the monitoring/alerting&lt;/p>
&lt;p>Embed the monitoring/alerting resources into the same module that you develop&lt;/p>
&lt;p>The approach we take largely depends on the resources that we are creating the monitoring for. Where possible, we try not to reinvent the wheel because, as you may have experienced, Terraform can be a little bit tricky to get right sometimes.&lt;/p>
&lt;p>I like to have this reusable component in place because I want the teams I work with to fall into the pit of success. That is, their experience will be better if they reuse the existing work and contribute to it where it makes sense. That said, I don’t believe there is a one-size-fits-all when it comes to any technology, so let’s quickly run down the benefits of each approach.&lt;/p>
&lt;h3 id="approach-1---separate-module">Approach 1 - Separate module&lt;/h3>
&lt;p>Using a separate module allows for the most flexibility but also takes much of the duplication out of our code. This means that the same alerts, such as CPU %, can be reused in various scenarios.&lt;/p>
&lt;p>However, this approach, whilst helpful, isn’t as optimal as I would personally like because it relies on two key things: discoverability and remembering to implement. Your company may have an excellent solution for discovering Terraform modules, but I’m yet to see a great implementation. I’d be very interested in speaking with you if you have a good implementation!&lt;/p>
&lt;h3 id="approach-2---embedded-within-the-module">Approach 2 - Embedded within the module&lt;/h3>
&lt;p>The second approach gives us the pit of success we are after because developers do not have to consider monitoring as the module embeds this with sensible defaults.&lt;/p>
&lt;p>I generally find the most optimal solution to combine the approaches mentioned above. We typically build the monitoring/alerting into a separate module and then embed that into the module that creates the resources we are interested in. This gives us the best of both worlds!&lt;/p>
&lt;h3 id="what-should-an-alert-look-like">What should an alert look like?&lt;/h3>
&lt;p>Getting alerts right can be a little tricky, but there are some key steps that you can take to make sure your alerts can be actioned appropriately:&lt;/p>
&lt;ol>
&lt;li>Actionable: It should clearly indicate what action should be taken in response to the alert.&lt;/li>
&lt;li>Timely: It should be triggered as close to the event as possible so that the appropriate action can be taken in a timely manner.&lt;/li>
&lt;li>Accurate: It should only be triggered when there is a real issue and not due to false positives.&lt;/li>
&lt;li>Specific: It should provide enough information to allow someone to understand and address the issue without requiring further investigation.&lt;/li>
&lt;li>Relevant: It should only be sent to people responsible for or able to take action on the issue.&lt;/li>
&lt;/ol>
&lt;p>With my teams, I always try to look at it from the perspective of “how would I want to deal with this at 2 am after a long week”. With this perspective, I find that my teams and I create a high standard of alerts. Couple this with the terraform module approach above, and you should take your Terraform to the next level!&lt;/p>
&lt;p>Before I leave you to get on with your week, I want to let you know about something happening pretty soon. I’m launching a &lt;a href="https://youtube.com/@DevOpsWithStu?sub_confirmation=1">DevOps-focused YouTube channel&lt;/a>! On the 20th Jan 2023, DevOpsWithStu will go live with three videos. This channel will be the same as this newsletter, with helpful tutorials on AWS, Terraform and general DevOps topics. If you like the content in this newsletter, I’m sure you’ll enjoy the content going out on YouTube, so hit that subscribe button &amp;amp; notification bell to get alerts for when new content is available!&lt;/p>
&lt;p>Side note: I also own the &lt;a href="https://youtube.com/@CodeWithStu?sub_confirmation=1">CodeWithStu YouTube channel&lt;/a>, which is focused on the .NET stack.&lt;/p>
&lt;p>&lt;strong>📣 Get the Weekly Newsletter Straight to Your Inbox! 📣&lt;/strong>&lt;/p>
&lt;p>Don&amp;rsquo;t miss out on the latest updates! Subscribe to the &lt;a href="https://news.codewithstu.tv">Observed! Newsletter&lt;/a> now and stay up-to-date with the latest tips and tricks across AWS, Devops and Architecture. &lt;a href="https://news.codewithstu.tv">Click here&lt;/a> to subscribe and start receiving your weekly dose of tech news!&lt;/p></description></item><item><title>Observed No. 1 - VPC Endpoint Policies</title><link>https://im5tu.io/article/2023/01/observed-no.-1-vpc-endpoint-policies/</link><pubDate>Mon, 02 Jan 2023 01:00:00 +0000</pubDate><guid>https://im5tu.io/article/2023/01/observed-no.-1-vpc-endpoint-policies/</guid><description>&lt;p>Welcome to the very first edition of Observed! Each week I bring you a tip you can implement in your infrastructure across many categories like AWS, Terraform and General DevOps practices. This week&amp;rsquo;s edition looks at VPC endpoint policies in AWS.&lt;/p>
&lt;h2 id="what-are-vpc-endpoints">What Are VPC Endpoints?&lt;/h2>
&lt;p>VPC endpoints are network interfaces you can create in your VPC to enable communication between your VPC and other AWS services without using an Internet gateway, VPN, or VPC peering. VPC Endpoints allow you to secure and control access to both AWS services and your services by:&lt;/p>
&lt;p>Enabling access to AWS services from within your VPC without requiring a NAT gateway or VPN connection.&lt;/p>
&lt;p>Enabling private connectivity between your VPC and other AWS services, such as Amazon S3, Amazon SQS, and Amazon SNS allows you to keep your data and communication within the AWS network, improving security and reducing data transfer costs.&lt;/p>
&lt;p>Enabling access to AWS services from on-premises networks using AWS Direct Connect allows you to create a secure, private connection between your on-premises network and your VPC and then use VPC endpoints to access AWS services without going over the Internet.&lt;/p>
&lt;p>By default, VPC endpoints allow full access to the resources they are created for, so we need to add policies to guard against unwanted actions. For example, if you create a VPC endpoint for SQS, then the endpoint will allow any SQS traffic over the network. This is where VPC endpoint policies come into play.&lt;/p>
&lt;h2 id="vpc-endpoint-policies">VPC Endpoint Policies&lt;/h2>
&lt;p>One overlooked factor of VPC endpoints is the policies you can attach to them. VPC endpoint policies are an optional series of rules to control access to your VPC endpoint, which are attached to the endpoint itself rather than to an individual resource or service. Some common use cases for VPC endpoint policies include:&lt;/p>
&lt;p>Allowing only specific AWS accounts to access your VPC endpoint ensuring that only authorised users can access it.&lt;/p>
&lt;p>Allowing only specific IAM users or roles to access your VPC endpoint, which is useful for controlling access on a more granular level, allowing you to grant or deny access to individual IAM users or roles.&lt;/p>
&lt;p>Allowing only specific VPCs to access your VPC endpoint. This can be useful for limiting access to your VPC endpoint to only specific VPCs, such as VPCs that belong to your organisation.&lt;/p>
&lt;p>The policies themselves follow the standard IAM policy format, with the slight difference that you should only reference resources for the specific type of VPC endpoint. For example, don&amp;rsquo;t try to apply SNS permissions on an SQS VPC endpoint.&lt;/p>
&lt;p>Let’s take a look at an example VPC endpoint policy:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-json" data-lang="json">&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Statement&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Sid&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;PreventUnintendedResourcesAndPrincipals&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Principal&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;*&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Action&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;s3:*&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Effect&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;Deny&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Resource&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;*&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;Condition&amp;#34;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;StringNotEquals&amp;#34;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;aws:ResourceOrgId&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;o-XXXXXXX&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;aws:PrincipalOrgId&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;o-XXXXXXX&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This policy prevents S3 usage outside the current organisation by using global conditional keys. When a principal makes a request to AWS, AWS gathers the request information into a request context. This request context is made available to you via the Condition element of the statement block in the policy document.&lt;/p>
&lt;p>The action section of the policy can either be wildcarded like I have in the example above, or you can limit it to specific actions such as s3:PutObject. When I create my policies, I try to use a combination of Effect:Deny and NotAction. I believe that being more specific about what actions are allowed on a VPC Endpoint leads to a better security posture.&lt;/p>
&lt;p>You can see which AWS services support VPC Endpoint Policies, and other valuable information, by using the describe-vpc-endpoint-services CLI command and checking for the field VpcEndpointPolicySupported in the response.&lt;/p>
&lt;p>I believe VPC Endpoint Policies are critical for securing infrastructure in sensitive environments, which is why they are part of my Well Architected Toolkit, which I’ll release later this year. Have you implemented VPC Endpoint Policies? What use cases have you found for them? Let me know how you use them below or reach out to me on &lt;a href="https://twitter.com/codewithstu">Twitter&lt;/a>.&lt;/p>
&lt;p>&lt;strong>📣 Get the Weekly Newsletter Straight to Your Inbox! 📣&lt;/strong>&lt;/p>
&lt;p>Don&amp;rsquo;t miss out on the latest updates! Subscribe to the &lt;a href="https://news.codewithstu.tv">Observed! Newsletter&lt;/a> now and stay up-to-date with the latest tips and tricks across AWS, Devops and Architecture. &lt;a href="https://news.codewithstu.tv">Click here&lt;/a> to subscribe and start receiving your weekly dose of tech news!&lt;/p></description></item><item><title>Live: Serverless SaaS</title><link>https://im5tu.io/video/live-serverless-saas/</link><pubDate>Thu, 01 Sep 2022 09:44:15 +0100</pubDate><guid>https://im5tu.io/video/live-serverless-saas/</guid><description>&lt;p>In this session, I&amp;rsquo;m going to build out a design for a serverless SaaS solution on AWS. This takes into account things like budgetting and technology choices. The aim will be to to deploy a regionally independent solution.&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube-nocookie.com/embed/cxZ2HllKUg0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div></description></item><item><title>Live AWS System Design: Payments Gateway</title><link>https://im5tu.io/video/live-aws-system-design-payments-gateway/</link><pubDate>Thu, 18 Aug 2022 10:08:30 +0100</pubDate><guid>https://im5tu.io/video/live-aws-system-design-payments-gateway/</guid><description>&lt;p>In this session, we are going to design a typically Payments Gateway that you would be expected to design as part of the hiring process for a FinTech. We will about some of the considerations to take into account when facing this question in a systems design interview.&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube-nocookie.com/embed/cSetznf9CWA" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div></description></item><item><title>Live: AWS Multi-Account Structure Design</title><link>https://im5tu.io/video/live-aws-multi-account-structure-design/</link><pubDate>Thu, 11 Aug 2022 09:20:59 +0100</pubDate><guid>https://im5tu.io/video/live-aws-multi-account-structure-design/</guid><description>&lt;p>In this session, we are going to design an AWS Multi-Account structure and the networking behind it. We will learn how to connect different accounts together and some of the considerations to take into account when facing this question in a systems design interview.&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube-nocookie.com/embed/DxxtGEekhJY" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div></description></item><item><title>Live: Systems Design - Stock Tick API</title><link>https://im5tu.io/video/live-systems-design-stock-tick-api/</link><pubDate>Thu, 04 Aug 2022 09:36:49 +0100</pubDate><guid>https://im5tu.io/video/live-systems-design-stock-tick-api/</guid><description>&lt;p>In this session, we are going to design a fairly typical stock trading API. We are going to go through some of the considerations that you need to have as well as looking at some of the curve balls that you may face. This will help you design distributed systems and data intensive applications using an event driven architecture.&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube-nocookie.com/embed/jSSsRMD3RUY" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div></description></item><item><title>How to set up OpenTelemetry Collector on Kubernetes</title><link>https://im5tu.io/video/how-to-set-up-opentelemetry-collector-on-kubernetes/</link><pubDate>Tue, 26 Jul 2022 12:00:07 +0100</pubDate><guid>https://im5tu.io/video/how-to-set-up-opentelemetry-collector-on-kubernetes/</guid><description>&lt;p>OpenTelemetry loves Kubernetes, which is clear to see from the OpenTelemetry Operator for Kubernetes. In this video, you will learn:&lt;/p>
&lt;ul>
&lt;li>How to install the OpenTelemetry Operator&lt;/li>
&lt;li>How to automatically inject a sidecar into a pod&lt;/li>
&lt;li>How to create a new collector configuration&lt;/li>
&lt;li>How to add automatic instrumentation for supported languages&lt;/li>
&lt;/ul>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube-nocookie.com/embed/GJAhtrc5IbQ" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;h2 id="video-links">Video Links&lt;/h2>
&lt;ul>
&lt;li>[https://cert-manager.io/docs/installation/](Certificate Manager Installation)&lt;/li>
&lt;li>&lt;a href="https://github.com/open-telemetry/opentelemetry-operator/pull/976">OpenTelemetry Operator .NET Auto Instrumentation PR&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/open-telemetry/opentelemetry-operator/issues/756">OpenTelemetry Operator .NET Auto Instrumentation Issue&lt;/a>&lt;/li>
&lt;/ul></description></item></channel></rss>